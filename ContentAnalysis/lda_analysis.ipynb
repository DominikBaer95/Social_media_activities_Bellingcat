{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import json\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTextDF(df):\n",
    "    stopWords = stopwords.words('english')\n",
    "    lemma  = WordNetLemmatizer()\n",
    "    df['text'] = df['text'].str.replace(\"([^0-9a-zA-ZäöüÄÖÜß \\t])|(\\w+:\\/\\/\\S+)\", \"\", regex=True)\n",
    "    df['text'] = df['text'].str.replace(\"[0-9]+[a-z]*\", \"\", regex=True)\n",
    "    df['text'] = df['text'].apply(lambda rawText: rawText.lower().split())\n",
    "    df['text'] = df['text'].apply(lambda splittedText: [word for word in splittedText if not word in stopWords])\n",
    "    df['text'] = df['text'].apply(lambda splittedText: ' '.join([str(item) for item in splittedText]))\n",
    "    df['text'] = df['text'].apply(lambda splittedText: ' '.join(lemma.lemmatize(word) for word in splittedText.split()))\n",
    "    return df\n",
    "    \n",
    "def generateVectorizedDF(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    corpus = df['text'].to_list()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    columns_name = vectorizer.get_feature_names()\n",
    "    # index = df_text_analysis['created_at'].to_list()\n",
    "    data = X.toarray()\n",
    "    return pd.DataFrame(data=data, columns=columns_name)\n",
    "    # return pd.DataFrame(data=data, index= index, columns=columns_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT DATA / PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../bellingcat_grouped_conversation_inclu_warPeriod_Final_lang_mode_thread.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_no_RT_only_thread = df[(df['lang'] == 'en') | (df['lang'] == 'und')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete stopwords, wordNetLemmatizer, numbers and transform to lower case\n",
    "df_text_analysis = generateTextDF(df_en_no_RT_only_thread.copy())\n",
    "# generating a vectorized dataframe for LDA-Analysis\n",
    "df_vectorized = generateVectorizedDF(df_text_analysis.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the count of every word to get the topwords\n",
    "sum = df_vectorized.sum(axis=0)\n",
    "sum.name = \"sum\"\n",
    "df_vectorized = df_vectorized.append(sum)\n",
    "df_vectorized.drop(['bellingcat', 'via', 'bellingcats'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "top_N_words = df_vectorized.iloc[-1, :].sort_values(ascending=False).iloc[0:N]\n",
    "bar = plt.bar(top_N_words.index, top_N_words.values)\n",
    "plt.title(f'Top_{N}_words')\n",
    "plt.bar_label(bar, rotation='vertical', padding=5)\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(top=2500)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA-ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cv = df_vectorized.columns.to_list()\n",
    "cv_arr = df_vectorized.iloc[:-1,:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=200, random_state=20)\n",
    "\n",
    "X_topics = lda_model.fit_transform(cv_arr)\n",
    "\n",
    "topic_words = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topics = []\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    relevant_words = np.array(vocab_cv)[sorted_topic_dist]\n",
    "    print(\"Topic\", str(i+1), relevant_words[:-(n_top_words+1):-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic = lda_model.transform(cv_arr)\n",
    "which_topic_for_which_tweet = []\n",
    "for n in range(doc_topic.shape[0]):\n",
    "\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    which_topic_for_which_tweet.append(topic_doc+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"{round((which_topic_for_which_tweet.count(i+1) * 100) / len(which_topic_for_which_tweet), 2)}% of the threads are assigned to topic {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(which_topic_for_which_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_en_no_RT_only_thread[['text', 'conversation_id', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.insert(3, \"Topic\", which_topic_for_which_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_excel('../bellingcat_tweets_with_topic.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "92c6157a433797f433e80c5b8542eb47b20ef3ee248b9e28a2229743b12ebd1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
